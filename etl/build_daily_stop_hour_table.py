import argparse
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
from pathlib import Path
from urllib.request import urlopen
from urllib.parse import quote
import xml.etree.ElementTree as ET
import pandas as pd
from datetime import datetime
import sqlite3
import os
import io
import sys
try:
    import boto3
except Exception:
    boto3 = None

def q(x):  
    return quote(str(x), safe="")

def fetch(url: str) -> str:
    with urlopen(url) as resp:
        return resp.read().decode("utf-8")

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def getenv_default(name: str, default=None):
    v = os.getenv(name)
    return v if v not in (None, "") else default

def ensure_audit_table_sqlite(conn):
    conn.execute("""
    CREATE TABLE IF NOT EXISTS etl_run_audit (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      job_name TEXT NOT NULL,
      target_date TEXT NOT NULL,  -- YYYYMMDD
      status TEXT NOT NULL,       -- OK / SKIPPED_SAME_DATE / NO_DATA / ERROR
      message TEXT,
      created_at TEXT NOT NULL    -- ISO timestamp
    );
    """)
    conn.commit()

def get_last_target_date_sqlite(conn, job_name: str):
    cur = conn.cursor()
    cur.execute("""
      SELECT target_date
      FROM etl_run_audit
      WHERE job_name = ?
      ORDER BY id DESC
      LIMIT 1
    """, (job_name,))
    row = cur.fetchone()
    return row[0] if row else None

def append_audit_sqlite(conn, job_name: str, target_date: str, status: str, message: str = None):
    ts = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
    conn.execute("""
      INSERT INTO etl_run_audit (job_name, target_date, status, message, created_at)
      VALUES (?, ?, ?, ?, ?)
    """, (job_name, target_date, status, message, ts))
    conn.commit()

# ---- S3 Upload Helper ----
def s3_upload_bytes(bucket: str, key: str, data: bytes, content_type: str = "text/csv"):
    """
    Upload raw bytes to S3. Requires that boto3 is available and credentials/role are configured.
    """
    if not boto3:
        print("[warn] boto3 not available; skip S3 upload.")
        return
    s3 = boto3.client("s3")
    s3.put_object(Bucket=bucket, Key=key, Body=data, ContentType=content_type)
    print(f"âœ… uploaded to s3://{bucket}/{key}")

# -----------------------------
# 1) tpssStationRouteTurn (ëŒ€ìš©ëŸ‰, ë‚ ì§œë§Œ íŒŒë¼ë¯¸í„° / route í•„í„°ëŠ” ë¡œì»¬ì—ì„œ)
# -----------------------------
def fetch_tpss_for_date(api_key: str, yyyymmdd: str, raw_dir: Path, page_size=1000):
    base = f"http://openapi.seoul.go.kr:8088/{q(api_key)}/xml/tpssStationRouteTurn"
    start, end = 1, page_size
    rows = []
    ensure_dir(raw_dir)

    # ì²« í˜ì´ì§€
    url = f"{base}/{q(start)}/{q(end)}/{q(yyyymmdd)}"
    xml = fetch(url)
    (raw_dir / f"{yyyymmdd}_{start}_{end}.xml").write_text(xml, encoding="utf-8")
    root = ET.fromstring(xml)
    total = int(root.findtext("list_total_count", default="0"))

    def parse_rows(root_elem):
        for r in root_elem.findall("row"):
            rec = {
                "date": r.findtext("CRTR_DD", ""),
                "rte_id": r.findtext("RTE_ID", ""),
                "stop_id": r.findtext("STOPS_ID", ""),
                "stop_seq": r.findtext("STOPS_SEQ", ""),
                "bus_opr_day": r.findtext("BUS_OPR", "0")
            }
            # 0~23ì‹œê°„ ìš´í–‰íšŸìˆ˜
            for h in range(24):
                tag = f"BUS_OPR_{h:02d}"
                v = r.findtext(tag)
                rec[f"BUS_OPR_{h:02d}"] = int(v) if v and v.strip().isdigit() else 0
            yield rec

    rows.extend(list(parse_rows(root)))

    # ë‹¤ìŒ í˜ì´ì§€ë“¤
    while end < total:
        start = end + 1
        end = min(start + page_size - 1, total)
        url = f"{base}/{q(start)}/{q(end)}/{q(yyyymmdd)}"
        xml = fetch(url)
        (raw_dir / f"{yyyymmdd}_{start}_{end}.xml").write_text(xml, encoding="utf-8")
        root = ET.fromstring(xml)
        rows.extend(list(parse_rows(root)))

    df = pd.DataFrame(rows)
    # ì •ìˆ˜/ë¬¸ì ì •ë¦¬
    if df.empty:
        return df
    df["stop_seq"] = pd.to_numeric(df["stop_seq"], errors="coerce")
    for h in range(24):
        df[f"BUS_OPR_{h:02d}"] = pd.to_numeric(df[f"BUS_OPR_{h:02d}"], errors="coerce").fillna(0).astype(int)
    return df

def melt_tpss_hourly(df_tpss: pd.DataFrame) -> pd.DataFrame:
    if df_tpss.empty:
        return df_tpss
    hour_cols = [f"BUS_OPR_{h:02d}" for h in range(24)]
    df = df_tpss.melt(
        id_vars=["date","rte_id","stop_id","stop_seq","bus_opr_day"],
        value_vars=hour_cols,
        var_name="hour_tag", value_name="ìš´í–‰íšŸìˆ˜"
    )
    df["ì‹œê°„"] = df["hour_tag"].str[-2:].astype(int)
    df = df.drop(columns=["hour_tag"])
    # ë‚ ì§œ í¬ë§·
    df["ê¸°ì¤€_ë‚ ì§œ"] = pd.to_datetime(df["date"], format="%Y%m%d").dt.strftime("%Y-%m-%d")
    return df[["ê¸°ì¤€_ë‚ ì§œ","rte_id","stop_id","stop_seq","ì‹œê°„","ìš´í–‰íšŸìˆ˜"]]

# -----------------------------
# TPSS CSV ë¡œë“œ í—¬í¼
# -----------------------------
def load_tpss_from_csv(csv_path, route_no: str, rte_id: str, yyyymmdd: str, route_map_csv=None) -> pd.DataFrame:
    """
    ë¡œì»¬ CSV(ì •ë¥˜ì¥ë³„/ì‹œê°„ëŒ€ë³„ ë²„ìŠ¤ ìš´í–‰íšŸìˆ˜ í”¼ë²— í˜•íƒœ)ë¥¼ ì½ì–´
    [ê¸°ì¤€_ë‚ ì§œ, rte_id, stop_id, stop_seq, ì‹œê°„, ìš´í–‰íšŸìˆ˜] Long í˜•íƒœë¡œ ë³€í™˜.
    CSV ê¸°ëŒ€ ì»¬ëŸ¼ ì˜ˆ:
      - ê¸°ì¤€_ë‚ ì§œ (YYYYMMDD)
      - ë…¸ì„ _ID
      - ì •ë¥˜ì¥_ID
      - ë²„ìŠ¤ìš´í–‰íšŸìˆ˜_ì¼
      - ë²„ìŠ¤ìš´í–‰íšŸìˆ˜_00ì‹œ ... ë²„ìŠ¤ìš´í–‰íšŸìˆ˜_23ì‹œ
      - ì •ë¥˜ì¥_ìˆœì„œ
    """
    df = pd.read_csv(csv_path, dtype=str)
    # ë‚ ì§œ í•„í„°
    df = df[df["ê¸°ì¤€_ë‚ ì§œ"] == yyyymmdd].copy()
    if df.empty:
        return pd.DataFrame(columns=["ê¸°ì¤€_ë‚ ì§œ","rte_id","stop_id","stop_seq","ì‹œê°„","ìš´í–‰íšŸìˆ˜"])

    # route_key(ë…¸ì„ _ID) ë§¤í•‘: route_map_csvì—ì„œ (RTE_NO, RTE_ID) â†’ ë…¸ì„ _ID
    route_key = None
    if route_map_csv:
        try:
            rm = pd.read_csv(route_map_csv, dtype=str)
            hit = rm[(rm["RTE_NO"].astype(str) == str(route_no)) & (rm["RTE_ID"].astype(str) == str(rte_id))]
            if not hit.empty:
                route_key = hit.iloc[0]["ë…¸ì„ _ID"]
        except FileNotFoundError:
            route_key = None
        except Exception:
            route_key = None

    if route_key is not None:
        df = df[df["ë…¸ì„ _ID"].astype(str) == str(route_key)].copy()

    # ì‹œê°„ ì»¬ëŸ¼ ìˆ˜ì§‘
    hour_cols = [c for c in df.columns if c.startswith("ë²„ìŠ¤ìš´í–‰íšŸìˆ˜_") and c.endswith("ì‹œ")]
    # ì•ˆì „í•˜ê²Œ 00~23ì‹œë§Œ ì •ë ¬
    def _hkey(c): 
        try:
            return int(c.split("_")[-1].replace("ì‹œ",""))
        except:
            return 99
    hour_cols = sorted(hour_cols, key=_hkey)

    # ê²°ì¸¡/ë¹ˆì¹¸ â†’ 0
    for c in hour_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(int)

    # melt
    long_df = df.melt(
        id_vars=["ê¸°ì¤€_ë‚ ì§œ","ì •ë¥˜ì¥_ID","ì •ë¥˜ì¥_ìˆœì„œ"],
        value_vars=hour_cols,
        var_name="hour_tag", value_name="ìš´í–‰íšŸìˆ˜"
    )
    long_df["ì‹œê°„"] = long_df["hour_tag"].str.extract(r"(\d+)").astype(int)
    long_df = long_df.drop(columns=["hour_tag"])

    # í˜•ì‹ ë§ì¶”ê¸°
    long_df = long_df.rename(columns={"ì •ë¥˜ì¥_ID":"stop_id","ì •ë¥˜ì¥_ìˆœì„œ":"stop_seq"})
    long_df["rte_id"] = str(rte_id)
    # ë‚ ì§œ í¬ë§· ë³€ê²½
    long_df["ê¸°ì¤€_ë‚ ì§œ"] = pd.to_datetime(long_df["ê¸°ì¤€_ë‚ ì§œ"], format="%Y%m%d").dt.strftime("%Y-%m-%d")

    # ì—´ ì •ë¦¬/ìˆœì„œ
    long_df = long_df[["ê¸°ì¤€_ë‚ ì§œ","rte_id","stop_id","stop_seq","ì‹œê°„","ìš´í–‰íšŸìˆ˜"]].copy()
    # íƒ€ì… ì •ë¦¬
    long_df["stop_seq"] = pd.to_numeric(long_df["stop_seq"], errors="coerce")
    long_df["ìš´í–‰íšŸìˆ˜"] = pd.to_numeric(long_df["ìš´í–‰íšŸìˆ˜"], errors="coerce").fillna(0).astype(int)
    return long_df

# -----------------------------
# 2) CardBusStatisticsServiceNew (ì¼ë³„ ì •ë¥˜ì¥ ì´ ìŠ¹/í•˜ì°¨ + ì •ë¥˜ì¥ëª…)
# -----------------------------
def fetch_daily_stats_for_route(api_key: str, yyyymmdd: str, route_no: str, raw_dir: Path):
    base = f"http://openapi.seoul.go.kr:8088/{q(api_key)}/xml/CardBusStatisticsServiceNew"
    url = f"{base}/{q(1)}/{q(500)}/{q(yyyymmdd)}/{q(route_no)}/"
    xml = fetch(url)
    ensure_dir(raw_dir)
    (raw_dir / f"{yyyymmdd}_{route_no}.xml").write_text(xml, encoding="utf-8")
    root = ET.fromstring(xml)

    rows = []
    for row in root.findall("row"):
        reg_ymd = row.findtext("REG_YMD","")
        rows.append({
            "date": row.findtext("USE_YMD",""),
            "rte_id": row.findtext("RTE_ID",""),
            "route_no": row.findtext("RTE_NO",""),
            "stop_id": row.findtext("STOPS_ID",""),
            "stop_name": row.findtext("SBWY_STNS_NM",""),
            "board_total_day": int(row.findtext("GTON_TNOPE","0") or 0),
            "alight_total_day": int(row.findtext("GTOFF_TNOPE","0") or 0),
            "reg_ymd": reg_ymd
        })
    df = pd.DataFrame(rows)
    if df.empty:
        return df
    df["ê¸°ì¤€_ë‚ ì§œ"] = pd.to_datetime(df["date"], format="%Y%m%d").dt.strftime("%Y-%m-%d")
    return df[["ê¸°ì¤€_ë‚ ì§œ","rte_id","route_no","stop_id","stop_name","board_total_day","alight_total_day","reg_ymd"]]

# -----------------------------
# 3) CardBusTimeNew (ì›”ë³„ ì •ë¥˜ì¥ ì‹œê°„ëŒ€ ìŠ¹/í•˜ì°¨ ë¶„í¬ â†’ ê°€ì¤‘ì¹˜)
# -----------------------------
def fetch_monthly_time_for_route(api_key: str, yyyymm: str, route_no: str, raw_dir: Path):
    base = f"http://openapi.seoul.go.kr:8088/{q(api_key)}/xml/CardBusTimeNew"
    url = f"{base}/{q(1)}/{q(500)}/{q(yyyymm)}/{q(route_no)}/"
    xml = fetch(url)
    ensure_dir(raw_dir)
    (raw_dir / f"{yyyymm}_{route_no}.xml").write_text(xml, encoding="utf-8")
    root = ET.fromstring(xml)

    def get_hour(row, h, on=True):
        tags = [f"HR_{h}_GET_ON_TNOPE", f"HR_{h}_GET_ON_NOPE"] if on else [f"HR_{h}_GET_OFF_TNOPE", f"HR_{h}_GET_OFF_NOPE"]
        for t in tags:
            v = row.findtext(t)
            if v is not None:
                try:
                    return int(v)
                except:
                    return 0
        return 0

    rows = []
    for r in root.findall("row"):
        base_rec = {
            "use_ym": r.findtext("USE_YM",""),
            "route_no": r.findtext("RTE_NO",""),
            "stop_id": r.findtext("STOPS_ID",""),
            "stop_name": r.findtext("SBWY_STNS_NM",""),
            "reg_ymd": r.findtext("REG_YMD","")
        }
        for h in range(24):
            rows.append({
                **base_rec,
                "ì‹œê°„": h,
                "m_board": get_hour(r, h, True),
                "m_alight": get_hour(r, h, False)
            })
    df = pd.DataFrame(rows)
    return df

def build_hour_weights(df_m: pd.DataFrame) -> pd.DataFrame:
    if df_m.empty:
        return df_m
    g = df_m.groupby(["route_no","stop_id"], as_index=False)[["m_board","m_alight"]].sum().rename(columns={"m_board":"sum_board","m_alight":"sum_alight"})
    df = df_m.merge(g, on=["route_no","stop_id"], how="left")
    # ê°€ì¤‘ì¹˜(í•©=1). í•©ì´ 0ì´ë©´ ê· ë“± ë¶„ë°°
    df["w_board"] = df.apply(lambda r: (r["m_board"]/r["sum_board"]) if r["sum_board"]>0 else (1/24), axis=1)
    df["w_alight"] = df.apply(lambda r: (r["m_alight"]/r["sum_alight"]) if r["sum_alight"]>0 else (1/24), axis=1)
    return df[["route_no","stop_id","ì‹œê°„","w_board","w_alight"]]

# -----------------------------
# DB helpers (SQLite)
# -----------------------------
def connect_sqlite(db_path: str):
    conn = sqlite3.connect(db_path)
    conn.execute("PRAGMA foreign_keys = ON;")
    return conn

def create_dim_fact_tables_sqlite(conn):
    # Create tables with SQLite syntax
    ddl = [
        """
        CREATE TABLE IF NOT EXISTS dim_route_map (
          route_key TEXT PRIMARY KEY,
          rte_id    TEXT NOT NULL,
          route_no  TEXT NOT NULL
        );
        """,
        """
        CREATE TABLE IF NOT EXISTS dim_stop_link (
          route_key TEXT NOT NULL,
          stop_id   TEXT NOT NULL,
          link_distance_m REAL,
          stop_seq  INTEGER,
          PRIMARY KEY(route_key, stop_id, stop_seq)
        );
        """,
        """
        CREATE TABLE IF NOT EXISTS fact_demand_daily_stop (
          date TEXT NOT NULL,
          rte_id TEXT,
          route_no TEXT,
          stop_id TEXT,
          stop_name TEXT,
          board_total INTEGER,
          alight_total INTEGER,
          reg_ymd TEXT,
          UNIQUE (date, route_no, stop_id)
        );
        """,
        """
        CREATE TABLE IF NOT EXISTS fact_demand_monthly_hourly_stop (
          use_ym TEXT NOT NULL,
          route_no TEXT,
          stop_id TEXT,
          stop_name TEXT,
          hour INTEGER,
          board_total INTEGER,
          alight_total INTEGER,
          reg_ymd TEXT,
          UNIQUE (use_ym, route_no, stop_id, hour)
        );
        """
    ]
    cur = conn.cursor()
    for stmt in ddl:
        cur.execute(stmt)
    conn.commit()

def create_flat_table_if_not_exists(conn, table: str):
    # Flat table for final output (SQLite syntax)
    ddl = f"""
    CREATE TABLE IF NOT EXISTS {table} (
      ê¸°ì¤€_ë‚ ì§œ TEXT NOT NULL,
      ë…¸ì„ ID TEXT,
      ì •ë¥˜ì¥_ID TEXT NOT NULL,
      ì‹œê°„ INTEGER NOT NULL,
      ìŠ¹ì°¨ì¸ì› REAL,
      í•˜ì°¨ì¸ì› REAL,
      ë…¸ì„ ë²ˆí˜¸ TEXT,
      ì—­ëª… TEXT,
      ì •ë¥˜ì¥_ìˆœì„œ INTEGER,
      ìš´í–‰íšŸìˆ˜ INTEGER,
      ë§í¬_êµ¬ê°„ê±°ë¦¬_m REAL,
      UNIQUE (ê¸°ì¤€_ë‚ ì§œ, ë…¸ì„ ë²ˆí˜¸, ì •ë¥˜ì¥_ID, ì‹œê°„)
    );
    """
    cur = conn.cursor()
    cur.execute(ddl)
    conn.commit()
# -----------------------------
# 4) ë§í¬ê±°ë¦¬ & ë…¸ì„ ID ë§¤í•‘
# -----------------------------
def create_dim_fact_tables_mysql(conn):
    ddl = """
    CREATE TABLE IF NOT EXISTS dim_route_map (
      route_key VARCHAR(64) PRIMARY KEY,
      rte_id    VARCHAR(64) NOT NULL,
      route_no  VARCHAR(32) NOT NULL,
      INDEX idx_route_map_rte (rte_id),
      INDEX idx_route_map_no  (route_no)
    ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

    CREATE TABLE IF NOT EXISTS dim_stop_link (
      route_key VARCHAR(64) NOT NULL,
      stop_id   VARCHAR(64) NOT NULL,
      link_distance_m DOUBLE,
      stop_seq  INT,
      PRIMARY KEY(route_key, stop_id, stop_seq)
    ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

    CREATE TABLE IF NOT EXISTS fact_demand_daily_stop (
      date DATE NOT NULL,
      rte_id VARCHAR(64),
      route_no VARCHAR(32),
      stop_id VARCHAR(64),
      stop_name VARCHAR(255),
      board_total INT,
      alight_total INT,
      reg_ymd DATE,
      UNIQUE KEY uq_daily (date, route_no, stop_id),
      INDEX idx_daily_route_date (route_no, date),
      INDEX idx_daily_stop (stop_id)
    ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

    CREATE TABLE IF NOT EXISTS fact_demand_monthly_hourly_stop (
      use_ym CHAR(6) NOT NULL,
      route_no VARCHAR(32),
      stop_id VARCHAR(64),
      stop_name VARCHAR(255),
      hour TINYINT UNSIGNED,
      board_total INT,
      alight_total INT,
      reg_ymd DATE,
      UNIQUE KEY uq_monthly (use_ym, route_no, stop_id, hour),
      INDEX idx_monthly_route_hour (route_no, use_ym, hour),
      INDEX idx_monthly_stop (stop_id)
    ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
    """
    with conn.cursor() as cur:
        for stmt in ddl.split(";"):
            s = stmt.strip()
            if s:
                cur.execute(s + ";")
    conn.commit()

# SQLite upsert helpers
def upsert_dim_route_map_sqlite(conn, route_map_csv: Path):
    if not route_map_csv.exists():
        print(f"[warn] route map CSV not found: {route_map_csv} (skip)")
        return
    df = pd.read_csv(route_map_csv)
    df = df.rename(columns={"ë…¸ì„ _ID":"route_key", "RTE_ID":"rte_id", "RTE_NO":"route_no"})
    df["route_key"] = df["route_key"].astype(str)
    df["rte_id"] = df["rte_id"].astype(str)
    df["route_no"] = df["route_no"].astype(str)
    sql = """
    INSERT OR REPLACE INTO dim_route_map (route_key, rte_id, route_no)
    VALUES (?, ?, ?)
    """
    data = list(df[["route_key","rte_id","route_no"]].itertuples(index=False, name=None))
    cur = conn.cursor()
    cur.executemany(sql, data)
    conn.commit()

def upsert_dim_stop_link_sqlite(conn, link_csv: Path):
    df = pd.read_csv(link_csv)
    df = df.rename(columns={"ë…¸ì„ _ID":"route_key","ì •ë¥˜ì¥_ID":"stop_id","ë§í¬_êµ¬ê°„ê±°ë¦¬(m)":"link_distance_m","ì •ë¥˜ì¥_ìˆœì„œ":"stop_seq"})
    df["route_key"] = df["route_key"].astype(str)
    df["stop_id"] = df["stop_id"].astype(str)
    sql = """
    INSERT OR REPLACE INTO dim_stop_link (route_key, stop_id, link_distance_m, stop_seq)
    VALUES (?, ?, ?, ?)
    """
    data = list(df[["route_key","stop_id","link_distance_m","stop_seq"]].itertuples(index=False, name=None))
    cur = conn.cursor()
    cur.executemany(sql, data)
    conn.commit()

def upsert_dim_stop_link_from_master_sqlite(conn, stop_master_df: pd.DataFrame, route_key: str | None):
    if stop_master_df is None or stop_master_df.empty:
        return
    df = stop_master_df.copy()
    # ê¸°ëŒ€ ì»¬ëŸ¼: route_key(ì—†ìœ¼ë©´ ì±„ì›€), stop_id, link_distance_m, link_stop_seq
    if "stop_id" not in df.columns or "link_stop_seq" not in df.columns:
        return
    if "link_distance_m" not in df.columns:
        return
    if "route_key" not in df.columns:
        df["route_key"] = route_key if route_key is not None else None
    df = df.dropna(subset=["stop_id","link_stop_seq"])
    tuples = list(df[["route_key","stop_id","link_distance_m","link_stop_seq"]]
                    .rename(columns={"link_stop_seq":"stop_seq"})
                    .itertuples(index=False, name=None))
    if not tuples:
        return
    sql = """
    INSERT OR REPLACE INTO dim_stop_link (route_key, stop_id, link_distance_m, stop_seq)
    VALUES (?, ?, ?, ?)
    """
    conn.executemany(sql, tuples)
    conn.commit()

def upsert_fact_demand_daily_sqlite(conn, df_daily: pd.DataFrame):
    if df_daily is None or df_daily.empty:
        return
    tmp = df_daily.rename(columns={
        "ê¸°ì¤€_ë‚ ì§œ":"date",
        "rte_id":"rte_id",
        "route_no":"route_no",
        "stop_id":"stop_id",
        "stop_name":"stop_name",
        "board_total_day":"board_total",
        "alight_total_day":"alight_total",
        "reg_ymd":"reg_ymd"
    }).copy()
    try:
        tmp["reg_ymd"] = pd.to_datetime(tmp["reg_ymd"], format="%Y%m%d", errors="coerce").dt.strftime("%Y-%m-%d")
    except Exception:
        pass
    sql = """
    INSERT OR REPLACE INTO fact_demand_daily_stop
    (date, rte_id, route_no, stop_id, stop_name, board_total, alight_total, reg_ymd)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """
    data = list(tmp[["date","rte_id","route_no","stop_id","stop_name","board_total","alight_total","reg_ymd"]].itertuples(index=False, name=None))
    cur = conn.cursor()
    cur.executemany(sql, data)
    conn.commit()

def upsert_fact_demand_monthly_hourly_sqlite(conn, df_monthly: pd.DataFrame):
    if df_monthly is None or df_monthly.empty:
        return
    tmp = df_monthly.rename(columns={
        "use_ym":"use_ym",
        "route_no":"route_no",
        "stop_id":"stop_id",
        "stop_name":"stop_name",
        "ì‹œê°„":"hour",
        "m_board":"board_total",
        "m_alight":"alight_total",
        "reg_ymd":"reg_ymd"
    }).copy()
    try:
        tmp["reg_ymd"] = pd.to_datetime(tmp["reg_ymd"], format="%Y%m%d", errors="coerce").dt.strftime("%Y-%m-%d")
    except Exception:
        pass
    sql = """
    INSERT OR REPLACE INTO fact_demand_monthly_hourly_stop
    (use_ym, route_no, stop_id, stop_name, hour, board_total, alight_total, reg_ymd)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """
    data = list(tmp[["use_ym","route_no","stop_id","stop_name","hour","board_total","alight_total","reg_ymd"]].itertuples(index=False, name=None))
    cur = conn.cursor()
    cur.executemany(sql, data)
    conn.commit()

def upsert_fact_ops_hourly_sqlite(conn, df_tpss: pd.DataFrame):
    """
    Store hourly ops data from tpss_long into fact_ops_hourly_stop table.
    """
    if df_tpss is None or df_tpss.empty:
        return
    tmp = df_tpss.rename(columns={
        "ê¸°ì¤€_ë‚ ì§œ":"date",
        "rte_id":"rte_id",
        "stop_id":"stop_id",
        "stop_seq":"stop_seq",
        "ì‹œê°„":"hour",
        "ìš´í–‰íšŸìˆ˜":"ops"
    }).copy()
    sql = """
    CREATE TABLE IF NOT EXISTS fact_ops_hourly_stop (
      date TEXT NOT NULL,
      rte_id TEXT,
      stop_id TEXT,
      stop_seq INTEGER,
      hour INTEGER,
      ops INTEGER,
      UNIQUE (date, rte_id, stop_id, hour)
    );
    """
    conn.execute(sql)
    data = list(tmp[["date","rte_id","stop_id","stop_seq","hour","ops"]].itertuples(index=False, name=None))
    conn.executemany("""
      INSERT OR REPLACE INTO fact_ops_hourly_stop
      (date, rte_id, stop_id, stop_seq, hour, ops)
      VALUES (?, ?, ?, ?, ?, ?)
    """, data)
    conn.commit()

def upsert_dataframe(conn, table: str, df: pd.DataFrame):
    # Column order aligned with the final output schema
    cols = [
        "ê¸°ì¤€_ë‚ ì§œ","ë…¸ì„ ID","ì •ë¥˜ì¥_ID","ì‹œê°„",
        "ìŠ¹ì°¨ì¸ì›","í•˜ì°¨ì¸ì›","ë…¸ì„ ë²ˆí˜¸","ì—­ëª…",
        "ì •ë¥˜ì¥_ìˆœì„œ","ìš´í–‰íšŸìˆ˜","ë§í¬_êµ¬ê°„ê±°ë¦¬(m)"
    ]
    # Replace NaN with None for DB insertion
    df2 = df[cols].where(pd.notnull(df[cols]), None)

    # Rename column for SQLite compatibility (ë§í¬_êµ¬ê°„ê±°ë¦¬(m) â†’ ë§í¬_êµ¬ê°„ê±°ë¦¬_m)
    insert_cols = [c if c != "ë§í¬_êµ¬ê°„ê±°ë¦¬(m)" else "ë§í¬_êµ¬ê°„ê±°ë¦¬_m" for c in cols]
    col_list = ", ".join(insert_cols)
    placeholders = ", ".join(["?"] * len(cols))

    # Data for upsert: rename column in DataFrame for SQLite
    df2 = df2.rename(columns={"ë§í¬_êµ¬ê°„ê±°ë¦¬(m)": "ë§í¬_êµ¬ê°„ê±°ë¦¬_m"})
    data = [tuple(row[c if c != "ë§í¬_êµ¬ê°„ê±°ë¦¬(m)" else "ë§í¬_êµ¬ê°„ê±°ë¦¬_m"] for c in cols) for _, row in df2.iterrows()]
    if not data:
        return
    sql = f"INSERT OR REPLACE INTO {table} ({col_list}) VALUES ({placeholders})"
    cur = conn.cursor()
    cur.executemany(sql, data)
    conn.commit()

# -----------------------------
# 4) ë§í¬ê±°ë¦¬ & ë…¸ì„ ID ë§¤í•‘
# -----------------------------
def load_link_distance(link_csv: Path) -> pd.DataFrame:
    df = pd.read_csv(link_csv)
    # ê¸°ëŒ€ ì»¬ëŸ¼: ë…¸ì„ _ID,ì •ë¥˜ì¥_ID,ë§í¬_êµ¬ê°„ê±°ë¦¬(m),ì •ë¥˜ì¥_ìˆœì„œ
    df = df.rename(columns={"ì •ë¥˜ì¥_ID":"stop_id","ë§í¬_êµ¬ê°„ê±°ë¦¬(m)":"link_distance_m","ì •ë¥˜ì¥_ìˆœì„œ":"link_stop_seq"})
    df["stop_id"] = df["stop_id"].astype(str)
    df["ë…¸ì„ _ID"] = df["ë…¸ì„ _ID"].astype(str)
    return df

def load_route_map(route_map_csv: Path) -> pd.DataFrame:
    # ê¸°ëŒ€ ì»¬ëŸ¼: ë…¸ì„ _ID, RTE_ID, RTE_NO
    if not route_map_csv.exists():
        print(f"[warn] route map CSV not found: {route_map_csv} (skip; ë§í¬ê±°ë¦¬ ë§¤ì¹­ì€ ë…¸ì„ _ID ì§ì ‘ ì§€ì • í•„ìš”)")
        return pd.DataFrame()
    df = pd.read_csv(route_map_csv)
    df["ë…¸ì„ _ID"] = df["ë…¸ì„ _ID"].astype(str)
    df["RTE_ID"] = df["RTE_ID"].astype(str)
    df["RTE_NO"] = df["RTE_NO"].astype(str)
    return df

def load_stop_master(stop_master_csv: Path, route_no: str, rte_id: str, yyyymmdd: str) -> pd.DataFrame:
    """
    ìœ ì—° ë¡œë”: ì—´ ì´ë¦„ì´ ë‹¬ë¼ë„ ìµœëŒ€í•œ ë§¤ì¹­.
    ë°˜í™˜ ì»¬ëŸ¼: ['route_key','stop_id','link_distance_m','link_stop_seq']
    ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ YYYYMMDD(ì˜ˆ: 20250801)ë¡œ í•„í„°ë§.
    """
    if not stop_master_csv or not Path(stop_master_csv).exists():
        return pd.DataFrame(columns=["route_key","stop_id","link_distance_m","link_stop_seq"])

    df = pd.read_csv(stop_master_csv, dtype=str)

    def pick(*names):
        for n in names:
            if n in df.columns:
                return n
        return None

    col_route_key = pick("ë…¸ì„ _ID","ROUTE_KEY","ROUTE_ID")
    col_rte_id    = pick("RTE_ID","ë…¸ì„ ì•„ì´ë””")
    col_route_no  = pick("RTE_NO","ë…¸ì„ ë²ˆí˜¸","ROUTE_NO")
    col_stop_id   = pick("ì •ë¥˜ì¥_ID","STOPS_ID","STOP_ID")
    col_stop_seq  = pick("ì •ë¥˜ì¥_ìˆœì„œ","STOPS_SEQ","STOP_SEQ")
    col_dist      = pick("ë§í¬_êµ¬ê°„ê±°ë¦¬(m)","LINK_DISTANCE_M","ë§í¬ê±°ë¦¬","LINK_DIST_M")
    col_date      = pick("CRTR_DD","ê¸°ì¤€_ë‚ ì§œ","DATE")

    if col_stop_id is None or col_stop_seq is None:
        return pd.DataFrame(columns=["route_key","stop_id","link_distance_m","link_stop_seq"])

    # ë‚ ì§œ í•„í„°(ìˆìœ¼ë©´)
    if col_date is not None:
        d = df[col_date].astype(str).str.replace(r"[^0-9]","", regex=True)
        df = df[d == str(yyyymmdd)]

    # ë…¸ì„  í•„í„°(ê°€ëŠ¥í•˜ë©´)
    if col_rte_id is not None:
        df = df[df[col_rte_id].astype(str) == str(rte_id)]
    elif col_route_no is not None:
        df = df[df[col_route_no].astype(str) == str(route_no)]

    if df.empty:
        return pd.DataFrame(columns=["route_key","stop_id","link_distance_m","link_stop_seq"])

    out = pd.DataFrame()
    out["route_key"] = df[col_route_key].astype(str) if col_route_key else None
    out["stop_id"] = df[col_stop_id].astype(str)
    out["link_stop_seq"] = pd.to_numeric(df[col_stop_seq], errors="coerce")
    out["link_distance_m"] = pd.to_numeric(df[col_dist], errors="coerce") if col_dist else pd.NA
    out = out.dropna(subset=["stop_id","link_stop_seq"])
    return out

# -----------------------------
# ë©”ì¸ ì¡°ë¦½
# -----------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--stats-key", required=True, help="CardBusStatistics/Time API Key")
    ap.add_argument("--tpss-key", required=True, help="tpssStationRouteTurn API Key")
    ap.add_argument("--route-no", required=True, help="ë…¸ì„ ë²ˆí˜¸ (ì˜ˆ: 172)")
    ap.add_argument("--rte-id", required=True, help="RTE_ID (ì˜ˆ: 11110028)")
    ap.add_argument("--date", required=True, help="YYYYMMDD (ì˜ˆ: 20250601)")
    ap.add_argument("--month", required=True, help="YYYYMM (ì˜ˆ: 202506)")
    ap.add_argument("--link-csv", default=None,
                help="(ì„ íƒ) ë…¸ì„ _ID,ì •ë¥˜ì¥_ID,ë§í¬_êµ¬ê°„ê±°ë¦¬(m),ì •ë¥˜ì¥_ìˆœì„œ CSV. stop-masterì— ë§í¬ê±°ë¦¬ê°€ ìˆìœ¼ë©´ ìƒëµ ê°€ëŠ¥")
    ap.add_argument("--stop-master-csv", default=None,
                help="(ì„ íƒ) ë…¸ì„  ì •ë¥˜ì¥ ë§ˆìŠ¤í„° CSV ê²½ë¡œ (ë…¸ì„ _ID, ì •ë¥˜ì¥_ID, ì •ë¥˜ì¥_ìˆœì„œ, [ë§í¬_êµ¬ê°„ê±°ë¦¬(m)], [CRTR_DD] í¬í•¨ ê°€ëŠ¥)")
    ap.add_argument("--route-map-csv", default=None, help="(ì„ íƒ) ë…¸ì„ _ID=RTE_ID ê°€ì • ì‹œ ë¶ˆí•„ìš”. ì œê³µ ì‹œ ë…¸ì„ _ID,RTE_ID,RTE_NO ë§¤í•‘ CSV")
    ap.add_argument("--db-path", required=True, help="SQLite DB file path")
    ap.add_argument("--db-table", default="daily_stop_hour", help="Target table name (default: daily_stop_hour)")
    ap.add_argument("--tpss-csv", default=None, help="(ì„ íƒ) tpssStationRouteTurnì„ ëŒ€ì‹ í•  ë¡œì»¬ CSV ê²½ë¡œ")
    ap.add_argument("--out-csv", default=None)
    ap.add_argument("--s3-bucket", default=getenv_default("S3_BUCKET"), help="(ì„ íƒ) ì—…ë¡œë“œí•  S3 ë²„í‚·ëª…. í™˜ê²½ë³€ìˆ˜ S3_BUCKET ì‚¬ìš© ê°€ëŠ¥")
    ap.add_argument("--s3-prefix", default=getenv_default("S3_PREFIX", "processed"), help="(ì„ íƒ) S3 í‚¤ prefix (ê¸°ë³¸: processed)")
    args = ap.parse_args()

    db_path = args.db_path
    db_table = args.db_table

    # ---- Audit: skip if target date equals previous run's target ----
    job_name = getenv_default("JOB_NAME", "build_daily_stop_hour_table")
    # We'll open a lightweight connection first only for audit check
    conn_audit = connect_sqlite(db_path)
    ensure_audit_table_sqlite(conn_audit)
    last_target = get_last_target_date_sqlite(conn_audit, job_name)
    if last_target is not None and str(last_target) == str(args.date):
        msg = f"[error] target date {args.date} is identical to previous run; skip execution."
        print(msg, file=sys.stderr)
        append_audit_sqlite(conn_audit, job_name, str(args.date), "SKIPPED_SAME_DATE", msg)
        conn_audit.close()
        return
    # keep this audit connection to append final status later if you prefer, or close it now:
    conn_audit.close()

    d_yyyy_mm_dd = datetime.strptime(args.date, "%Y%m%d").strftime("%Y-%m-%d")

    # ë…¸ì„ _ID == RTE_ID ê°€ì • (ì„œìš¸ì‹œ ë‚´ í†µì¼)
    route_key = str(args.rte_id)
    route_map = pd.DataFrame()  # ë§¤í•‘ CSV ì—†ì´ ì§„í–‰
    # Prefer stop-master for link/sequence if provided (date-filtered)
    stop_master_df = load_stop_master(Path(args.stop_master_csv), args.route_no, args.rte_id, args.date) if args.stop_master_csv else pd.DataFrame()

    # 1) TPSìŠ¤(ëŒ€ìš©ëŸ‰) ìˆ˜ì§‘/ë¡œë“œ
    conn = connect_sqlite(db_path)
    # ensure audit table exists on the main connection too (so we can log outcomes)
    ensure_audit_table_sqlite(conn)
    # Ensure fact_ops_hourly_stop exists
    conn.execute("""
    CREATE TABLE IF NOT EXISTS fact_ops_hourly_stop (
      date TEXT NOT NULL,
      rte_id TEXT,
      stop_id TEXT,
      stop_seq INTEGER,
      hour INTEGER,
      ops INTEGER,
      UNIQUE (date, rte_id, stop_id, hour)
    );
    """)

    yyyy_mm_dd_str = datetime.strptime(args.date, "%Y%m%d").strftime("%Y-%m-%d")

    if args.tpss_csv:
        # ğŸš€ ë¹ ë¥¸ ê²½ë¡œ: ë¡œì»¬ CSV ì‚¬ìš©
        tpss_long = load_tpss_from_csv(args.tpss_csv, args.route_no, args.rte_id, args.date, args.route_map_csv)
        if tpss_long.empty:
            print("[warn] TPSS CSVì—ì„œ í•´ë‹¹ ë‚ ì§œ/ë…¸ì„  ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            upsert_fact_ops_hourly_sqlite(conn, tpss_long)
    else:
        # ê¸°ì¡´ ê²½ë¡œ: API í˜¸ì¶œ â†’ DB ìºì‹œ
        cur = conn.cursor()
        cur.execute("SELECT COUNT(1) FROM fact_ops_hourly_stop WHERE date=? AND rte_id=?", (yyyy_mm_dd_str, str(args.rte_id)))
        tpss_exists = cur.fetchone()[0] > 0
        if tpss_exists:
            # Load from DB
            tpss_long = pd.read_sql_query(
                "SELECT date as ê¸°ì¤€_ë‚ ì§œ, rte_id, stop_id, stop_seq, hour as ì‹œê°„, ops as ìš´í–‰íšŸìˆ˜ FROM fact_ops_hourly_stop WHERE date=? AND rte_id=?",
                conn, params=(yyyy_mm_dd_str, str(args.rte_id))
            )
        else:
            # Fetch from API, process, and store in DB
            tpss_raw = fetch_tpss_for_date(args.tpss_key, args.date, Path('data/raw/api/tpss'))
            tpss_raw = tpss_raw[tpss_raw['rte_id'].astype(str) == str(args.rte_id)]
            tpss_long = melt_tpss_hourly(tpss_raw)
            upsert_fact_ops_hourly_sqlite(conn, tpss_long)

    # ---- Hard filter to the single target route (rte_id) ----
    if 'tpss_long' in locals() and tpss_long is not None and not tpss_long.empty:
        tpss_long = tpss_long[tpss_long['rte_id'].astype(str) == str(args.rte_id)]

    # 2) ì¼ë³„ ì •ë¥˜ì¥ ì´ ìŠ¹/í•˜ì°¨ + ì •ë¥˜ì¥ëª…
    daily_stats = fetch_daily_stats_for_route(args.stats_key, args.date, args.route_no, Path("data/raw/api/CardBusStatisticsServiceNew"))
    # 3) ì›”ë³„ ì‹œê°„ëŒ€ ë¶„í¬ â†’ ê°€ì¤‘ì¹˜
    monthly = fetch_monthly_time_for_route(args.stats_key, args.month, args.route_no, Path("data/raw/api/CardBusTimeNew"))
    weights = build_hour_weights(monthly)  # [route_no, stop_id, ì‹œê°„, w_board, w_alight]

    # 4) ë§í¬ê±°ë¦¬ & ë¼ìš°íŠ¸ ë§¤í•‘ (stop-master ìš°ì„ , ì—†ìœ¼ë©´ link-csv)
    if stop_master_df is not None and not stop_master_df.empty:
        # stop_master_df columns: route_key, stop_id, link_distance_m, link_stop_seq
        link_df = stop_master_df.copy()
    else:
        # fallback to legacy link CSV (ë…¸ì„ _ID, ì •ë¥˜ì¥_ID, ë§í¬_êµ¬ê°„ê±°ë¦¬(m), ì •ë¥˜ì¥_ìˆœì„œ)
        link_df = load_link_distance(Path(args.link_csv))  # returns: [ë…¸ì„ _ID, stop_id, link_distance_m, link_stop_seq]

    # route_keyëŠ” RTE_IDì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©
    # route_key = str(args.rte_id)

    # 5) ì¡°ë¦½: fact_ops_hourly_stop(ì‹œê°„ëŒ€ ìš´í–‰íšŸìˆ˜) + ì¼ë³„ ì´ëŸ‰ + ì›” ê°€ì¤‘ì¹˜
    #    - ì‹œê°„ ë‹¨ìœ„ì— ìŠ¹/í•˜ì°¨ ë¶„ë°°
    #    - ì •ë¥˜ì¥ëª… ë¶™ì´ê¸°
    if tpss_long.empty:
        print("tpss ë°ì´í„°ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
        try:
            append_audit_sqlite(conn, getenv_default("JOB_NAME", "build_daily_stop_hour_table"), str(args.date), "NO_DATA", "tpss empty")
        except Exception:
            pass
        conn.close()
        return

    # ì •ë¥˜ì¥ëª…(ì¼ë³„) ë¶™ì´ê¸°
    name_map = (
        daily_stats[["ê¸°ì¤€_ë‚ ì§œ","stop_id","stop_name","route_no"]].drop_duplicates()
        if not daily_stats.empty
        else pd.DataFrame(columns=["ê¸°ì¤€_ë‚ ì§œ","stop_id","stop_name","route_no"])
    )
    df = tpss_long.merge(name_map, on=["ê¸°ì¤€_ë‚ ì§œ","stop_id"], how="left")

    # ---- Hard filter to single route: route_no & rte_id ----
    df["route_no"] = str(args.route_no)  # ë³´ì •
    df = df[(df["rte_id"].astype(str) == str(args.rte_id)) & (df["route_no"].astype(str) == str(args.route_no))]

    # ì¼ë³„ ì´ëŸ‰ ë¶™ì´ê¸°
    daily_map = daily_stats[["ê¸°ì¤€_ë‚ ì§œ","stop_id","board_total_day","alight_total_day"]].drop_duplicates() if not daily_stats.empty else None
    if daily_map is not None and not daily_map.empty:
        df = df.merge(daily_map, on=["ê¸°ì¤€_ë‚ ì§œ","stop_id"], how="left")
    else:
        df["board_total_day"] = 0
        df["alight_total_day"] = 0

    # --- ì›” ê°€ì¤‘ì¹˜ ì¤€ë¹„ (ë¹ˆ ê²½ìš°/route_no ëˆ„ë½ ì‹œ ê· ë“± ê°€ì¤‘ì¹˜ ìƒì„±) ---
    weights = build_hour_weights(monthly)  # ê¸°ëŒ€ ìŠ¤í‚¤ë§ˆ: [route_no, stop_id, ì‹œê°„, w_board, w_alight]

    # ì›” ê°€ì¤‘ì¹˜ê°€ ë¹„ì—ˆê±°ë‚˜, route_no ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜, í‚¤ê°€ ë§ì§€ ì•Šìœ¼ë©´ ê· ë“±(1/24)ë¡œ ìƒì„±
    if (weights is None) or weights.empty or ("stop_id" not in weights.columns) or ("ì‹œê°„" not in weights.columns):
        # tpss_longì—ì„œ ì‚¬ìš©ë˜ëŠ” ì •ë¥˜ì¥ê³¼ 0~23ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ê· ë“± ê°€ì¤‘ì¹˜ êµ¬ì„±
        stops = sorted(tpss_long["stop_id"].astype(str).unique().tolist()) if not tpss_long.empty else []
        hours = list(range(24))
        weights = pd.DataFrame(
            [(str(args.route_no), s, h, 1/24, 1/24) for s in stops for h in hours],
            columns=["route_no","stop_id","ì‹œê°„","w_board","w_alight"]
        )
    else:
        # íƒ€ì… ì •ê·œí™”: ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì•ˆì „í•˜ê²Œ ìºìŠ¤íŒ…
        if "route_no" in weights.columns:
            weights["route_no"] = weights["route_no"].astype(str)
        weights["stop_id"] = weights["stop_id"].astype(str)

    # ì›” ê°€ì¤‘ì¹˜ ë¶™ì´ê¸° (route_noê°€ ìˆìœ¼ë©´ route_noë„ í‚¤ë¡œ, ì—†ìœ¼ë©´ stop_id+ì‹œê°„ë§Œ)
    df["route_no"] = str(args.route_no)
    if "route_no" in weights.columns:
        df = df.merge(weights, on=["route_no","stop_id","ì‹œê°„"], how="left")
    else:
        df = df.merge(weights[["stop_id","ì‹œê°„","w_board","w_alight"]], on=["stop_id","ì‹œê°„"], how="left")

    # ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ê²½ìš°(merge í›„ ê²°ì¸¡)ë„ ê· ë“± ë¶„ë°°
    df["w_board"] = df["w_board"].fillna(1/24)
    df["w_alight"] = df["w_alight"].fillna(1/24)

    # ì‹œê°„ëŒ€ ìŠ¹/í•˜ì°¨ ì¶”ì •
    df["ìŠ¹ì°¨ì¸ì›"] = df["board_total_day"].fillna(0) * df["w_board"]
    df["í•˜ì°¨ì¸ì›"] = df["alight_total_day"].fillna(0) * df["w_alight"]

    # ë§í¬ê±°ë¦¬ ë¶™ì´ê¸° (ê°€ëŠ¥í•˜ë©´)
    if route_key is not None:
        # route_key ì»¬ëŸ¼ëª… ì°¨ì´ë¥¼ í¡ìˆ˜
        if "route_key" in link_df.columns:
            link_sub = link_df[link_df["route_key"].astype(str) == str(route_key)].copy()
        elif "ë…¸ì„ _ID" in link_df.columns:
            link_sub = link_df[link_df["ë…¸ì„ _ID"].astype(str) == str(route_key)].copy()
        else:
            link_sub = link_df.copy()

        # ìš°ì„  stop_idë¡œ ë¶™ì´ê³ , ì—†ìœ¼ë©´ seqë¡œ ë³´ì¡° ë§¤ì¹­
        if "stop_id" in link_sub.columns:
            df = df.merge(link_sub[["stop_id","link_distance_m"]], on="stop_id", how="left")
        # ë³´ì¡°: stop_seq == link_stop_seqë¡œ ë§¤ì¹­ (ì•„ì§ ë¹ˆ ê³³ë§Œ)
        # link_distance_m ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ë§Œë“¤ì–´ ë‘”ë‹¤
        if "link_distance_m" not in df.columns:
            df["link_distance_m"] = pd.NA
        missing = df["link_distance_m"].isna()
        if missing.any() and "link_stop_seq" in link_sub.columns:
            # ì•ˆì „í•œ ë§¤í•‘: link_stop_seq â†’ link_distance_m (ì¤‘ë³µì€ í‰ê·  ì²˜ë¦¬)
            tmp_map = (
                link_sub[["link_stop_seq", "link_distance_m"]]
                .dropna(subset=["link_stop_seq"])
                .copy()
            )
            # íƒ€ì… ì •ê·œí™”
            tmp_map["link_stop_seq"] = pd.to_numeric(tmp_map["link_stop_seq"], errors="coerce")
            # í‰ê· ê°’ìœ¼ë¡œ ì¤‘ë³µ ì¶•ì•½
            seq2dist = tmp_map.groupby("link_stop_seq")["link_distance_m"].mean()
            # ê²°ì¸¡ ëŒ€ìƒì—ë§Œ ë§¤í•‘
            df.loc[missing, "link_distance_m"] = df.loc[missing, "stop_seq"].map(seq2dist)

        # ë…¸ì„ ID ì»¬ëŸ¼ ì¶”ê°€
        df["ë…¸ì„ ID"] = str(route_key)
    else:
        # route_keyë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°ë¼ë„ stop_id ê¸°ì¤€ìœ¼ë¡œë§Œ ì‹œë„
        if "stop_id" in link_df.columns and "link_distance_m" in link_df.columns:
            df = df.merge(link_df[["stop_id","link_distance_m"]], on="stop_id", how="left")
        if "link_distance_m" not in df.columns:
            df["link_distance_m"] = pd.NA
        df["ë…¸ì„ ID"] = None

    # ìµœì¢… ì»¬ëŸ¼ êµ¬ì„±/ì •ë ¬
    df = df.rename(columns={
        "stop_id":"ì •ë¥˜ì¥_ID",
        "stop_name":"ì—­ëª…",
        "stop_seq":"ì •ë¥˜ì¥_ìˆœì„œ",
        "route_no":"ë…¸ì„ ë²ˆí˜¸",
        "ìš´í–‰íšŸìˆ˜":"ìš´í–‰íšŸìˆ˜",
        "link_distance_m":"ë§í¬_êµ¬ê°„ê±°ë¦¬(m)"
    })

    out_cols = ["ê¸°ì¤€_ë‚ ì§œ","ë…¸ì„ ID","ì •ë¥˜ì¥_ID","ì‹œê°„","ìŠ¹ì°¨ì¸ì›","í•˜ì°¨ì¸ì›","ë…¸ì„ ë²ˆí˜¸","ì—­ëª…","ì •ë¥˜ì¥_ìˆœì„œ","ìš´í–‰íšŸìˆ˜","ë§í¬_êµ¬ê°„ê±°ë¦¬(m)"]
    # ìš´í–‰íšŸìˆ˜ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ
    if "ìš´í–‰íšŸìˆ˜" not in df.columns:
        df["ìš´í–‰íšŸìˆ˜"] = 0
    # íƒ€ì… ì •ë¦¬
    df["ì •ë¥˜ì¥_ìˆœì„œ"] = pd.to_numeric(df["ì •ë¥˜ì¥_ìˆœì„œ"], errors="coerce").astype("Int64")
    df = df[out_cols].sort_values(["ê¸°ì¤€_ë‚ ì§œ","ì‹œê°„","ì •ë¥˜ì¥_ìˆœì„œ","ì •ë¥˜ì¥_ID"])

    # ---- Save to DB (normalized + flat) ----
    create_dim_fact_tables_sqlite(conn)
    # route_map CSVê°€ ì—†ë”ë¼ë„ ë…¸ì„ _ID == RTE_ID ê°€ì •ìœ¼ë¡œ dim_route_mapì— ì§ì ‘ upsert
    if args.route_map_csv and Path(args.route_map_csv).exists():
        upsert_dim_route_map_sqlite(conn, Path(args.route_map_csv))
    else:
        conn.execute("""
        CREATE TABLE IF NOT EXISTS dim_route_map (
          route_key TEXT PRIMARY KEY,
          rte_id    TEXT NOT NULL,
          route_no  TEXT NOT NULL
        );
        """)
        conn.execute("INSERT OR REPLACE INTO dim_route_map (route_key, rte_id, route_no) VALUES (?, ?, ?)",
                     (str(args.rte_id), str(args.rte_id), str(args.route_no)))
        conn.commit()
    # Prefer stop-master for link/sequence if available; otherwise fallback to --link-csv
    if stop_master_df is not None and not stop_master_df.empty:
        upsert_dim_stop_link_from_master_sqlite(conn, stop_master_df, route_key)
    elif args.link_csv:
        upsert_dim_stop_link_sqlite(conn, Path(args.link_csv))
    upsert_fact_demand_daily_sqlite(conn, daily_stats)
    upsert_fact_demand_monthly_hourly_sqlite(conn, monthly)
    create_flat_table_if_not_exists(conn, db_table)
    upsert_dataframe(conn, db_table, df)
    affected = len(df)
    conn.close()

    # ---- Upload to S3 (optional) ----
    if affected > 0 and args.s3_bucket:
        # Key: {prefix}/daily_stop_hour/route={route_no}/date={date}.csv
        prefix = (args.s3_prefix or "processed").rstrip("/")
        s3_key = f"{prefix}/daily_stop_hour/route={args.route_no}/date={args.date}.csv"
        buf = io.StringIO()
        df.to_csv(buf, index=False, encoding="utf-8-sig")
        s3_upload_bytes(args.s3_bucket, s3_key, buf.getvalue().encode("utf-8-sig"), content_type="text/csv; charset=utf-8")

    try:
        if affected > 0:
            append_audit_sqlite(conn, getenv_default("JOB_NAME", "build_daily_stop_hour_table"), str(args.date), "OK", f"rows={affected}")
        else:
            append_audit_sqlite(conn, getenv_default("JOB_NAME", "build_daily_stop_hour_table"), str(args.date), "NO_DATA", "no rows affected")
    except Exception:
        pass
    if affected > 0:
        print(f"âœ… upserted {affected:,} rows into `{db_table}` and refreshed normalized tables in SQLite DB {db_path}")
    else:
        print("âš ï¸ ìƒì„±ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°/í•„í„°(ë‚ ì§œ, ë…¸ì„ )ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # ---- Save to CSV (optional) ----
    if args.out_csv:
        ensure_dir(Path(args.out_csv).parent)
        df.to_csv(args.out_csv, index=False, encoding="utf-8-sig")
        print(f"âœ… saved CSV: {args.out_csv}")

if __name__ == "__main__":
    main()