import argparse
import os
from pathlib import Path
import sqlite3
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import pandas as pd
import time

try:
    import boto3
except Exception:
    boto3 = None


def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def save_xml(text: str, path: Path):
    ensure_dir(path.parent)
    path.write_text(text, encoding="utf-8")

def get_text(elem, tag, default=""):
    v = elem.findtext(tag)
    return v if v is not None else default

def to_int_safe(v, default=0):
    try:
        return int(str(v).strip())
    except Exception:
        return default

# ----------------------------
# API í˜¸ì¶œ
# ----------------------------
from urllib.request import urlopen
from urllib.parse import quote

def fetch_url(url: str, timeout: float = 10.0, max_retries: int = 3, backoff: float = 1.5) -> str:
    """
    í˜¸ì¶œ ì‹¤íŒ¨/íƒ€ì„ì•„ì›ƒ/ì¼ì‹œì  5xxë¥¼ ëŒ€ë¹„í•œ ì¬ì‹œë„ ìœ í‹¸.
    - timeout: ê° ì‹œë„ë³„ íƒ€ì„ì•„ì›ƒ(ì´ˆ)
    - max_retries: ì „ì²´ ì¬ì‹œë„ íšŸìˆ˜
    - backoff: ì§€ìˆ˜ ë°±ì˜¤í”„ ë°°ìˆ˜
    """
    attempt = 0
    last_err = None
    while attempt <= max_retries:
        try:
            with urlopen(url, timeout=timeout) as resp:
                text = resp.read().decode("utf-8")
                # ê°„ë‹¨ ë¬´ê²°ì„± ì²´í¬: RESULT/CODEê°€ ìˆê³ , ì—ëŸ¬ ì½”ë“œë©´ raise
                try:
                    root = ET.fromstring(text)
                    code = root.findtext("RESULT/CODE")
                    msg  = root.findtext("RESULT/MESSAGE")
                    if code and code != "INFO-000":
                        raise RuntimeError(f"API returned error code={code}, message={msg}")
                except ET.ParseError:
                    # ì¼ë¶€ ì‘ë‹µì€ ê³§ë°”ë¡œ <row>ë¡œ ì‹œì‘ ê°€ëŠ¥ â†’ íŒŒì‹± ì‹¤íŒ¨ëŠ” ìŠ¤í‚µ
                    pass
                return text
        except Exception as e:
            last_err = e
            attempt += 1
            if attempt > max_retries:
                break
            sleep_sec = backoff ** attempt
            print(f"[warn] fetch_url failed (attempt {attempt}/{max_retries}) url={url} err={e}. retry in {sleep_sec:.1f}s")
            time.sleep(sleep_sec)
    raise RuntimeError(f"fetch_url exhausted retries: url={url}, last_err={last_err}")

def _q(x):
    return quote(str(x), safe="")

# ----------------------------
# S3 helpers
# ----------------------------
def _s3_client():
    if boto3 is None:
        return None
    try:
        return boto3.client("s3")
    except Exception:
        return None

def _s3_upload_file(s3, bucket: str, local_path: Path, key: str):
    if s3 is None:
        print("[warn] boto3 unavailable; skip S3 upload")
        return
    try:
        s3.upload_file(str(local_path), bucket, key)
        print(f"ğŸ“¤ uploaded to s3://{bucket}/{key}")
    except Exception as e:
        print(f"[warn] S3 upload failed for {local_path}: {e}")

# ----------------------------
# ë…¸ì„  ë§¤í•‘ ì ì¬ í•¨ìˆ˜ ì¶”ê°€
# ----------------------------
def upsert_dim_route_map(conn, map_csv_path: Path):
    """
    route_key(ë…¸ì„ _ID) â†” rte_id/RTE_NO ë§¤í•‘ ì ì¬.
    CSV ì»¬ëŸ¼ ìš”êµ¬: ['ë…¸ì„ _ID','RTE_ID','RTE_NO']
    """
    if not map_csv_path.exists():
        print(f"[warn] route map CSV not found: {map_csv_path} (skip)")
        return
    df = pd.read_csv(map_csv_path)
    need = ["ë…¸ì„ _ID", "RTE_ID", "RTE_NO"]
    for c in need:
        if c not in df.columns:
            raise ValueError(f"route_key_map CSVì— '{c}' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    df = df.rename(columns={
        "ë…¸ì„ _ID": "route_key",
        "RTE_ID":  "rte_id",
        "RTE_NO":  "route_no"
    })
    # ë¬¸ìì—´ ì •ê·œí™”
    df["route_key"] = df["route_key"].astype(str).str.strip()
    df["rte_id"]    = df["rte_id"].astype(str).str.strip()
    df["route_no"]  = df["route_no"].astype(str).str.strip()

    tuples = list(df[["route_key","rte_id","route_no"]].itertuples(index=False, name=None))
    conn.executemany("""
        INSERT OR REPLACE INTO dim_route_map(route_key, rte_id, route_no)
        VALUES (?, ?, ?)
    """, tuples)

# ----------------------------
# íŒŒì„œ: CardBusStatisticsServiceNew (ì¼ë³„)
# URL: /xml/CardBusStatisticsServiceNew/1/500/{YYYYMMDD}/{RTE_NO}/
# ----------------------------
def fetch_parse_daily_stats(api_key: str, date_yyyymmdd: str, route_no: str, raw_dir: Path, timeout: float = 10.0, retries: int = 3):
    base = f"http://openapi.seoul.go.kr:8088/{_q(api_key)}/xml/CardBusStatisticsServiceNew"
    page_start, page_end = 1, 500
    rows = []
    # 1í˜ì´ì§€ í˜¸ì¶œ
    url = f"{base}/{_q(page_start)}/{_q(page_end)}/{_q(date_yyyymmdd)}/{_q(route_no)}/"
    xml = fetch_url(url, timeout=timeout, max_retries=retries)
    save_xml(xml, raw_dir / f"{date_yyyymmdd}.xml")

    root = ET.fromstring(xml)
    total = int(root.findtext("list_total_count", default="0"))
    if total == 0:
        print(f"[warn] no daily rows for date={date_yyyymmdd}, route_no={route_no}")

    def parse_rows(root_elem):
        for row in root_elem.findall("row"):
            yield {
                "date": get_text(row, "USE_YMD"),        # YYYYMMDD
                "rte_id": get_text(row, "RTE_ID"),
                "route_no": get_text(row, "RTE_NO"),
                "stop_id": get_text(row, "STOPS_ID"),
                "stop_name": get_text(row, "SBWY_STNS_NM"),
                "board_total": to_int_safe(get_text(row, "GTON_TNOPE")),
                "alight_total": to_int_safe(get_text(row, "GTOFF_TNOPE")),
                "reg_ymd": get_text(row, "REG_YMD"),
            }

    rows.extend(list(parse_rows(root)))

    # í˜ì´ì§• í•„ìš” ì‹œ(ì—¬ê¸°ì„  total<=500ì¼ ê°€ëŠ¥ì„± ë†’ì§€ë§Œ ë°©ì–´)
    while len(rows) < total:
        page_start = page_end + 1
        page_end = page_start + 499
        url = f"{base}/{_q(page_start)}/{_q(page_end)}/{_q(date_yyyymmdd)}/{_q(route_no)}/"
        xml = fetch_url(url, timeout=timeout, max_retries=retries)
        save_xml(xml, raw_dir / f"{date_yyyymmdd}_{page_start}_{page_end}.xml")
        root = ET.fromstring(xml)
        rows.extend(list(parse_rows(root)))

    return rows

# ----------------------------
# íŒŒì„œ: CardBusTimeNew (ì›”ë³„ ì‹œê°„ëŒ€)
# URL: /xml/CardBusTimeNew/1/500/{YYYYMM}/{RTE_NO}/
# ----------------------------
def fetch_parse_monthly_time(api_key: str, yyyymm: str, route_no: str, raw_dir: Path, timeout: float = 10.0, retries: int = 3):
    base = f"http://openapi.seoul.go.kr:8088/{_q(api_key)}/xml/CardBusTimeNew"
    page_start, page_end = 1, 500
    rows = []

    url = f"{base}/{_q(page_start)}/{_q(page_end)}/{_q(yyyymm)}/{_q(route_no)}/"
    xml = fetch_url(url, timeout=timeout, max_retries=retries)
    save_xml(xml, raw_dir / f"{yyyymm}.xml")

    root = ET.fromstring(xml)
    total = int(root.findtext("list_total_count", default="0"))
    if total == 0:
        print(f"[warn] no monthly rows for use_ym={yyyymm}, route_no={route_no}")

    def get_hour_value(row_elem, h, onoff="ON"):
        # ì¼ë¶€ í•„ë“œëŠ” TNOPE, ì¼ë¶€ëŠ” NOPEë¡œ ì°íˆëŠ” ì‚¬ë¡€ê°€ ìˆì–´ ë‘˜ ë‹¤ ë°©ì–´
        if onoff == "ON":
            tags = [f"HR_{h}_GET_ON_TNOPE", f"HR_{h}_GET_ON_NOPE"]
        else:
            tags = [f"HR_{h}_GET_OFF_TNOPE", f"HR_{h}_GET_OFF_NOPE"]
        for t in tags:
            v = row_elem.findtext(t)
            if v is not None:
                return to_int_safe(v)
        return 0

    def parse_rows(root_elem):
        for row in root_elem.findall("row"):
            base_rec = {
                "use_ym": get_text(row, "USE_YM"),          # YYYYMM
                "route_no": get_text(row, "RTE_NO"),
                "stop_id": get_text(row, "STOPS_ID"),
                "stop_name": get_text(row, "SBWY_STNS_NM"),
                "reg_ymd": get_text(row, "REG_YMD"),
            }
            # 0~23ì‹œ long ì •ê·œí™”
            for h in range(24):
                yield {
                    **base_rec,
                    "hour": h,
                    "board_total": get_hour_value(row, h, "ON"),
                    "alight_total": get_hour_value(row, h, "OFF"),
                }

    rows.extend(list(parse_rows(root)))

    # í˜ì´ì§• í•„ìš”ì‹œ
    while len(rows) < total * 24:  # í•œ rowë‹¹ 24ì‹œê°„ ë ˆì½”ë“œê°€ ìƒì„±ë¨
        page_start = page_end + 1
        page_end = page_start + 499
        url = f"{base}/{_q(page_start)}/{_q(page_end)}/{_q(yyyymm)}/{_q(route_no)}/"
        xml = fetch_url(url, timeout=timeout, max_retries=retries)
        save_xml(xml, raw_dir / f"{yyyymm}_{page_start}_{page_end}.xml")
        root = ET.fromstring(xml)
        rows.extend(list(parse_rows(root)))

    return rows

# ----------------------------
# DB ì´ˆê¸°í™” ë° ì ì¬
# ----------------------------
CREATE_TABLES_SQL = """
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;

CREATE TABLE IF NOT EXISTS dim_route_map (
  route_key TEXT PRIMARY KEY,  -- ë§í¬ CSVì˜ ë…¸ì„ _ID
  rte_id    TEXT NOT NULL,     -- ì„œìš¸ APIì˜ RTE_ID
  route_no  TEXT NOT NULL      -- ì„œìš¸ APIì˜ RTE_NO (ì˜ˆ: '172')
);
CREATE INDEX IF NOT EXISTS idx_route_map_rte ON dim_route_map(rte_id);
CREATE INDEX IF NOT EXISTS idx_route_map_no  ON dim_route_map(route_no);

CREATE TABLE IF NOT EXISTS dim_stop_link (
  route_key TEXT,
  stop_id   TEXT,
  link_distance_m REAL,
  stop_seq  INTEGER,
  PRIMARY KEY(route_key, stop_id, stop_seq)
);

CREATE TABLE IF NOT EXISTS fact_demand_daily_stop (
  date TEXT,            -- YYYYMMDD
  rte_id TEXT,
  route_no TEXT,
  stop_id TEXT,
  stop_name TEXT,
  board_total INTEGER,
  alight_total INTEGER,
  reg_ymd TEXT
);

CREATE INDEX IF NOT EXISTS idx_daily_route_date ON fact_demand_daily_stop(route_no, date);
CREATE INDEX IF NOT EXISTS idx_daily_stop ON fact_demand_daily_stop(stop_id);

CREATE TABLE IF NOT EXISTS fact_demand_monthly_hourly_stop (
  use_ym TEXT,          -- YYYYMM
  route_no TEXT,
  stop_id TEXT,
  stop_name TEXT,
  hour INTEGER,         -- 0~23
  board_total INTEGER,
  alight_total INTEGER,
  reg_ymd TEXT
);

CREATE INDEX IF NOT EXISTS idx_monthly_route_hour ON fact_demand_monthly_hourly_stop(route_no, use_ym, hour);
CREATE INDEX IF NOT EXISTS idx_monthly_stop ON fact_demand_monthly_hourly_stop(stop_id);
"""

def upsert_dim_stop_link(conn, link_csv_path: Path):
    df = pd.read_csv(link_csv_path)
    # ì»¬ëŸ¼ ëª… ë§¤í•‘ ë°©ì–´
    col_map = {
        "ë…¸ì„ _ID": "route_key",
        "ì •ë¥˜ì¥_ID": "stop_id",
        "ë§í¬_êµ¬ê°„ê±°ë¦¬(m)": "link_distance_m",
        "ì •ë¥˜ì¥_ìˆœì„œ": "stop_seq",
    }
    for k, v in col_map.items():
        if k not in df.columns:
            raise ValueError(f"ë§í¬ CSVì— '{k}' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    df = df.rename(columns=col_map)
    df["route_key"] = df["route_key"].astype(str)
    df["stop_id"] = df["stop_id"].astype(str)

    tuples = list(df[["route_key","stop_id","link_distance_m","stop_seq"]].itertuples(index=False, name=None))
    conn.executemany("""
        INSERT OR REPLACE INTO dim_stop_link (route_key, stop_id, link_distance_m, stop_seq)
        VALUES (?, ?, ?, ?)
    """, tuples)

def insert_daily(conn, rows):
    tuples = [(
        r["date"], r["rte_id"], r["route_no"], r["stop_id"], r["stop_name"],
        r["board_total"], r["alight_total"], r["reg_ymd"]
    ) for r in rows]
    conn.executemany("""
        INSERT INTO fact_demand_daily_stop
        (date, rte_id, route_no, stop_id, stop_name, board_total, alight_total, reg_ymd)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """, tuples)

def insert_monthly_hourly(conn, rows):
    tuples = [(
        r["use_ym"], r["route_no"], r["stop_id"], r["stop_name"], r["hour"],
        r["board_total"], r["alight_total"], r["reg_ymd"]
    ) for r in rows]
    conn.executemany("""
        INSERT INTO fact_demand_monthly_hourly_stop
        (use_ym, route_no, stop_id, stop_name, hour, board_total, alight_total, reg_ymd)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """, tuples)

# ----------------------------
# ë©”ì¸
# ----------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--api-key", required=True, help="ì„œìš¸ì—´ë¦°ë°ì´í„° API Key")
    ap.add_argument("--route-no", required=True, help="ë…¸ì„ ë²ˆí˜¸ (ì˜ˆ: 172)")
    ap.add_argument("--month", required=True, help="YYYYMM (ì˜ˆ: 202506)")
    ap.add_argument("--start-date", required=True, help="YYYY-MM-DD (ì˜ˆ: 2025-06-01)")
    ap.add_argument("--end-date", required=True, help="YYYY-MM-DD (ì˜ˆ: 2025-06-30)")
    ap.add_argument("--link-csv", default="data/external/link_distance.csv")
    ap.add_argument("--route-map-csv", default="data/external/route_key_map.csv",
                    help="ë…¸ì„ _ID(ë§í¬) â†” RTE_ID/RTE_NO(ì„œìš¸API) ë§¤í•‘ CSV")
    ap.add_argument("--db-path", default="db/seoul_bus_172.db")
    ap.add_argument("--dump-sql-path", default="sql/seoul_bus_172_dump.sql")
    ap.add_argument("--s3-bucket", default=os.getenv("S3_BUCKET"),
                    help="ì—…ë¡œë“œ ëŒ€ìƒ S3 ë²„í‚· (ì˜µì…˜). í™˜ê²½ë³€ìˆ˜ S3_BUCKET ì‚¬ìš© ê°€ëŠ¥")
    ap.add_argument("--s3-prefix", default=os.getenv("S3_PREFIX", "raw"),
                    help="S3 key prefix (ê¸°ë³¸: raw). í™˜ê²½ë³€ìˆ˜ S3_PREFIX ì‚¬ìš© ê°€ëŠ¥")
    ap.add_argument("--s3-upload-db", action="store_true",
                    help="SQLite DB íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ")
    ap.add_argument("--s3-upload-sql", action="store_true",
                    help="SQL dump íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ")
    ap.add_argument("--s3-upload-raw", action="store_true",
                    help="ìˆ˜ì§‘í•œ ì›ë³¸ XMLë“¤ì„ S3ì— ì—…ë¡œë“œ")
    ap.add_argument("--http-timeout", type=float, default=10.0, help="API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ(ì´ˆ)")
    ap.add_argument("--http-retries", type=int, default=3, help="API í˜¸ì¶œ ì¬ì‹œë„ íšŸìˆ˜")
    ap.add_argument("--http-backoff", type=float, default=1.5, help="ì¬ì‹œë„ ì§€ìˆ˜ ë°±ì˜¤í”„ ë°°ìˆ˜")
    args = ap.parse_args()

    # í´ë” ì¤€ë¹„
    raw_daily_dir = Path("data/raw/api/CardBusStatisticsServiceNew")
    raw_monthly_dir = Path("data/raw/api/CardBusTimeNew")
    ensure_dir(raw_daily_dir); ensure_dir(raw_monthly_dir)
    ensure_dir(Path("db")); ensure_dir(Path("sql"))

    # S3 client (optional)
    s3 = _s3_client()

    # DB ì¤€ë¹„
    conn = sqlite3.connect(args.db_path)
    conn.executescript(CREATE_TABLES_SQL)

    # 0) ë…¸ì„  ë§¤í•‘ ì ì¬ (optional)
    upsert_dim_route_map(conn, Path(args.route_map_csv))
    conn.commit()

    # 1) ë§í¬ê±°ë¦¬ ì ì¬
    upsert_dim_stop_link(conn, Path(args.link_csv))
    conn.commit()

    # 2) ì¼ë³„ ìˆ˜ìš”(ìŠ¹í•˜ì°¨ í•©, ì •ë¥˜ì¥ ë‹¨ìœ„)
    start = datetime.strptime(args.start_date, "%Y-%m-%d").date()
    end = datetime.strptime(args.end_date, "%Y-%m-%d").date()
    cur = start
    while cur <= end:
        yyyymmdd = cur.strftime("%Y%m%d")
        rows = fetch_parse_daily_stats(args.api_key, yyyymmdd, args.route_no, raw_daily_dir, timeout=args.http_timeout, retries=args.http_retries)
        if rows:
            insert_daily(conn, rows)
            conn.commit()
        cur += timedelta(days=1)

    # 3) ì›”ë³„ ì‹œê°„ëŒ€ ìˆ˜ìš”(ì •ë¥˜ì¥Â·ì‹œê°„)
    rows_m = fetch_parse_monthly_time(args.api_key, args.month, args.route_no, raw_monthly_dir, timeout=args.http_timeout, retries=args.http_retries)
    if rows_m:
        insert_monthly_hourly(conn, rows_m)
        conn.commit()

    # 4) ë¤í”„(.sql) ì €ì¥
    with open(args.dump_sql_path, "w", encoding="utf-8") as f:
        for line in conn.iterdump():
            f.write(f"{line}\n")

    # 5) S3 ì—…ë¡œë“œ (ì˜µì…˜)
    if args.s3_bucket:
        # DB íŒŒì¼ ì—…ë¡œë“œ
        if args.s3_upload_db and Path(args.db_path).exists():
            db_key = f"{args.s3_prefix.rstrip('/')}/sqlite/{Path(args.db_path).name}"
            _s3_upload_file(s3, args.s3_bucket, Path(args.db_path), db_key)

        # SQL dump ì—…ë¡œë“œ
        if args.s3_upload_sql and Path(args.dump_sql_path).exists():
            dump_key = f"{args.s3_prefix.rstrip('/')}/dump/{Path(args.dump_sql_path).name}"
            _s3_upload_file(s3, args.s3_bucket, Path(args.dump_sql_path), dump_key)

        # RAW XML ì—…ë¡œë“œ (ì¼ë³„/ì›”ë³„ ë””ë ‰í„°ë¦¬ì˜ *.xml ì „ì²´ ì—…ë¡œë“œ)
        if args.s3_upload_raw:
            for p in raw_daily_dir.rglob("*.xml"):
                rel = p.relative_to(raw_daily_dir)
                key = f"{args.s3_prefix.rstrip('/')}/api/CardBusStatisticsServiceNew/{rel.as_posix()}"
                _s3_upload_file(s3, args.s3_bucket, p, key)
            for p in raw_monthly_dir.rglob("*.xml"):
                rel = p.relative_to(raw_monthly_dir)
                key = f"{args.s3_prefix.rstrip('/')}/api/CardBusTimeNew/{rel.as_posix()}"
                _s3_upload_file(s3, args.s3_bucket, p, key)

    conn.close()
    print(f"SQLite DB: {args.db_path}")
    print(f"SQL Dump  : {args.dump_sql_path}")
    print("Raw XML   : data/raw/api/... ì— ì €ì¥ë¨")

if __name__ == "__main__":
    main()