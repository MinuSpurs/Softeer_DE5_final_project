# save as: /Users/minwoo/Desktop/softeer/data_engineering_course_materials/missions/final_project/LSTM_v1.py
# ëª©ì : ì •ë¥˜ìž¥Ã—ì‹œê°„ ìŠ¹ì°¨ ìˆ˜ìš” ì˜ˆì¸¡ (ê³µê¸‰ í”¼ì²˜ ì£¼ìž… ë²„ì „)

import os
import numpy as np
import pandas as pd
from pathlib import Path
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# --------------------------
# Paths
# --------------------------
DATA_CSV = "/Users/minwoo/Desktop/softeer/data_engineering_course_materials/missions/final_project/data/172_ì‹œê°„ëŒ€ë³„_ì •ë¥˜ìž¥_ë…¸ì„ _ì¶”ì •ìŠ¹í•˜ì°¨+ìš´í–‰íšŸìˆ˜+ìˆœì„œ+ë§í¬ê±°ë¦¬.csv"
OUT_PRED = "/Users/minwoo/Desktop/softeer/data_engineering_course_materials/missions/final_project/data/172_LSTM_preds_ì •ë¥˜ìž¥ë³„_ì‹œê°„ë³„_ìŠ¹í•˜ì°¨_ì˜ˆì¸¡_í˜¼ìž¡ë„+ì¶”ê°€ë°°ì°¨.csv"

# --------------------------
# Train/Test ê¸°ê°„
# --------------------------
TRAIN_START = "2025-06-01"
TRAIN_END   = "2025-06-19"  # í•™ìŠµ êµ¬ê°„ì˜ ë§ˆì§€ë§‰ ì´í‹€ì€ ê²€ì¦ìœ¼ë¡œ ì‚¬ìš©
VAL_START   = "2025-06-20"
VAL_END     = "2025-06-21"
TEST_START  = "2025-06-22"
TEST_END    = "2025-06-29"

SEQ_LEN = 24   # 24ì‹œê°„ ížˆìŠ¤í† ë¦¬ ìž…ë ¥
BATCH   = 256
EPOCHS  = 100
LR      = 1e-3

np.random.seed(42)
tf.random.set_seed(42)

# --------------------------
# 1) ë°ì´í„° ë¡œë“œ/ì •ë¦¬
# --------------------------
df = pd.read_csv(DATA_CSV)

need = ["ê¸°ì¤€_ë‚ ì§œ","ì‹œê°„","ì •ë¥˜ìž¥_ID","ì •ë¥˜ìž¥_ìˆœì„œ","ì—­ëª…","ìŠ¹ì°¨ì¸ì›","í•˜ì°¨ì¸ì›","ìš´í–‰íšŸìˆ˜"]
miss = [c for c in need if c not in df.columns]
if miss:
    raise ValueError(f"ìž…ë ¥ì— í•„ìš”í•œ ì»¬ëŸ¼ ì—†ìŒ: {miss}")

# timestamp ìƒì„±
df["ê¸°ì¤€_ë‚ ì§œ"] = pd.to_datetime(df["ê¸°ì¤€_ë‚ ì§œ"])
df["timestamp"] = df["ê¸°ì¤€_ë‚ ì§œ"] + pd.to_timedelta(df["ì‹œê°„"].astype(int), unit="h")

# ì •ë ¬
df = df.sort_values(["ì •ë¥˜ìž¥_ID","timestamp"]).reset_index(drop=True)

# í†µê³¼ë²„ìŠ¤ìˆ˜ = ì‹œê°„ëŒ€ë³„ ìš´í–‰íšŸìˆ˜
df["í†µê³¼ë²„ìŠ¤ìˆ˜"] = df["ìš´í–‰íšŸìˆ˜"].fillna(0).clip(lower=0)

# ìŒìˆ˜ ìŠ¹ì°¨/í•˜ì°¨ ë³´ì •(ì•ˆì „ í´ë¦½)
df["ìŠ¹ì°¨ì¸ì›"] = df["ìŠ¹ì°¨ì¸ì›"].fillna(0).clip(lower=0)
df["í•˜ì°¨ì¸ì›"] = df["í•˜ì°¨ì¸ì›"].fillna(0).clip(lower=0)

# ë§í¬ê±°ë¦¬ í”¼ì²˜
if "ë§í¬_êµ¬ê°„ê±°ë¦¬(m)" in df.columns:
    # 0ì€ ì‹¤ì§ˆì  ê²°ì¸¡ìœ¼ë¡œ ê°„ì£¼ â†’ í‰ê·  ëŒ€ì²´
    mean_nonzero = df.loc[df["ë§í¬_êµ¬ê°„ê±°ë¦¬(m)"].replace(0, np.nan).notna(),"ë§í¬_êµ¬ê°„ê±°ë¦¬(m)"] \
                     .replace(0, np.nan).mean()
    df["ë§í¬ê±°ë¦¬"] = df["ë§í¬_êµ¬ê°„ê±°ë¦¬(m)"].replace(0, np.nan).fillna(mean_nonzero)
else:
    df["ë§í¬ê±°ë¦¬"] = 300.0

# ì‹œê°„/ìš”ì¼/ì£¼ë§
df["hour"] = df["timestamp"].dt.hour
df["dow"]  = df["timestamp"].dt.dayofweek
df["is_wend"] = (df["dow"]>=5).astype(int)

# === ê³µê¸‰ í”¼ì²˜ ì¶”ê°€ ===
# headway(ë¶„) = 60/í†µê³¼ë²„ìŠ¤ìˆ˜, 0ë¶„ëª¨/ê²°ì¸¡ ê°€ë“œ + ì´ìƒì¹˜ ìº¡í•‘
df["headway_min"] = np.where(df["í†µê³¼ë²„ìŠ¤ìˆ˜"] > 0, 60.0 / df["í†µê³¼ë²„ìŠ¤ìˆ˜"], np.nan)
df["headway_min"] = df["headway_min"].fillna(60.0).clip(lower=2.0, upper=60.0)

# ëª©í‘œ ë²„ìŠ¤ë‹¹ íƒ‘ìŠ¹(ì¢Œì„ 33, 1.5ë°° ê¸°ì¤€)
CAPACITY = 33.0
TARGET_MULT = 1.5
df["target_per_bus"] = CAPACITY * TARGET_MULT  # 49.5 â‰ˆ 50

# ì •ë¥˜ìž¥ ìž„ë² ë”©ìš© ì¸ë±ìŠ¤
stop_ids = df["ì •ë¥˜ìž¥_ID"].astype(str).unique()
stop2idx = {sid:i for i, sid in enumerate(stop_ids)}
df["stop_idx"] = df["ì •ë¥˜ìž¥_ID"].astype(str).map(stop2idx)

# í‘œì : y_board = ìŠ¹ì°¨, y_alight = í•˜ì°¨ (ì‹œê°„Â·ì •ë¥˜ìž¥ ì´ëŸ‰)
df["y_board"]  = df["ìŠ¹ì°¨ì¸ì›"].astype(float)
df["y_alight"] = df["í•˜ì°¨ì¸ì›"].astype(float)

# Train/Val/Test ë§ˆìŠ¤í¬ (ì¢…ë£Œì¼ 23:59:59 í¬í•¨)
_end_train = pd.to_datetime(TRAIN_END) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
_end_val   = pd.to_datetime(VAL_END)   + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
_end_test  = pd.to_datetime(TEST_END)  + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)

train_mask = (
    (df["timestamp"] >= pd.to_datetime(TRAIN_START)) &
    (df["timestamp"] <= _end_train)
)

val_mask = (
    (df["timestamp"] >= pd.to_datetime(VAL_START)) &
    (df["timestamp"] <= _end_val)
)

test_mask = (
    (df["timestamp"] >= pd.to_datetime(TEST_START)) &
    (df["timestamp"] <= _end_test)
)

# --------------------------
# 2) ì‹œí€€ìŠ¤ ë¹Œë” (ì •ë¥˜ìž¥ë³„ë¡œ 24ì‹œê°„ ížˆìŠ¤í† ë¦¬ â†’ t ì‹œì  ì˜ˆì¸¡)
# --------------------------
# âœ… ê³µê¸‰ í”¼ì²˜ í¬í•¨
feat_cols = [
    "hour","dow","is_wend",
    "ì •ë¥˜ìž¥_ìˆœì„œ","ë§í¬ê±°ë¦¬",
    "í†µê³¼ë²„ìŠ¤ìˆ˜","headway_min","target_per_bus"
]

def make_sequences(frame):
    X_num = []
    X_stop = []
    y = []
    keys = []
    for sid, g in frame.groupby("ì •ë¥˜ìž¥_ID"):
        g = g.sort_values("timestamp").reset_index(drop=True)
        arr = g[feat_cols].to_numpy(dtype=float)
        sid_idx = g["stop_idx"].iloc[0]
        yy = g[["y_board","y_alight"]].to_numpy(dtype=float)

        for t in range(SEQ_LEN, len(g)):
            X_num.append(arr[t-SEQ_LEN:t])             # (T,F)
            X_stop.append([sid_idx]*SEQ_LEN)            # (T,)
            y.append(yy[t])                             # (2,)
            keys.append((sid, g["timestamp"].iloc[t]))

    return (
        np.array(X_num),                               # (N,T,F)
        np.array(X_stop, dtype=int),                   # (N,T)
        np.array(y, dtype=float),
        keys
    )

train_df = df[train_mask].copy()
val_df   = df[val_mask].copy()
test_df  = df[test_mask].copy()

Xnum_tr, Xstop_tr, y_tr, _      = make_sequences(train_df)
Xnum_val, Xstop_val, y_val, _   = make_sequences(val_df)
Xnum_te, Xstop_te, y_te, keys_te = make_sequences(test_df)

# ì •ê·œí™”(Train ê¸°ì¤€ MinMax)
mins = Xnum_tr.reshape(-1, Xnum_tr.shape[-1]).min(axis=0)
maxs = Xnum_tr.reshape(-1, Xnum_tr.shape[-1]).max(axis=0)
rng  = np.where((maxs-mins)==0, 1.0, (maxs-mins))

def norm_fn(x):
    return (x - mins) / rng

Xnum_tr  = norm_fn(Xnum_tr)
Xnum_val = norm_fn(Xnum_val)
Xnum_te  = norm_fn(Xnum_te)

# --------------------------
# 3) ëª¨ë¸ (ì •ë¥˜ìž¥ ìž„ë² ë”© + ìˆ˜ì¹˜í”¼ì²˜ â†’ LSTM)
# --------------------------
num_features = Xnum_tr.shape[-1]
n_stops = len(stop2idx)

inp_num  = layers.Input(shape=(SEQ_LEN, num_features))
inp_stop = layers.Input(shape=(SEQ_LEN,), dtype="int32")

emb = layers.Embedding(input_dim=n_stops, output_dim=8)(inp_stop)  # (B,T,8)
x = layers.Concatenate(axis=-1)([inp_num, emb])                     # (B,T,F+8)
x = layers.LSTM(64, return_sequences=False)(x)
x = layers.Dense(64, activation="relu")(x)
x = layers.Dropout(0.2)(x)
out = layers.Dense(2, activation="relu")(x)  # [ìŠ¹ì°¨, í•˜ì°¨] ë™ì‹œ ì˜ˆì¸¡

model = keras.Model([inp_num, inp_stop], out)
model.compile(optimizer=keras.optimizers.Adam(LR), loss="mae", metrics=["mse"])
model.summary()

# --------------------------
# 4) í•™ìŠµ
# --------------------------
callbacks = [
    keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-5, verbose=1),
    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=1)
]
model.fit([Xnum_tr, Xstop_tr], y_tr,
          validation_data=([Xnum_val, Xstop_val], y_val),
          epochs=EPOCHS, batch_size=BATCH, verbose=2,
          callbacks=callbacks)

# --------------------------
# 5) ì˜ˆì¸¡ â†’ CSV ìƒì„± (test ê¸°ê°„)
# --------------------------
y_hat = model.predict([Xnum_te, Xstop_te], batch_size=BATCH, verbose=0)  # (N,2)
y_hat_board  = y_hat[:,0]
y_hat_alight = y_hat[:,1]

# í‚¤ë¥¼ DFë¡œ
pred_df = pd.DataFrame(keys_te, columns=["ì •ë¥˜ìž¥_ID","timestamp"])
pred_df["ì •ë¥˜ìž¥_ID"] = pred_df["ì •ë¥˜ìž¥_ID"].astype(str)
pred_df["ê¸°ì¤€_ë‚ ì§œ"] = pred_df["timestamp"].dt.strftime("%Y-%m-%d")
pred_df["ì‹œê°„"] = pred_df["timestamp"].dt.hour

# ë©”íƒ€ ë¶™ì´ê¸°(ìˆœì„œ/ì—­ëª…/í†µê³¼ë²„ìŠ¤ìˆ˜) - ê°™ì€ ì‹œì  í–‰ê³¼ ì¡°ì¸
join_cols = ["ì •ë¥˜ìž¥_ID","ê¸°ì¤€_ë‚ ì§œ","ì‹œê°„"]
meta = test_df[join_cols + ["ì •ë¥˜ìž¥_ìˆœì„œ","ì—­ëª…","í†µê³¼ë²„ìŠ¤ìˆ˜"]].drop_duplicates()

# ðŸ”§ í‚¤ íƒ€ìž… ì •ê·œí™”: ì •ë¥˜ìž¥_ID/ê¸°ì¤€_ë‚ ì§œ/ì‹œê°„ì˜ dtypeì„ ë§žì¶˜ë‹¤
meta = meta.copy()
meta["ì •ë¥˜ìž¥_ID"] = meta["ì •ë¥˜ìž¥_ID"].astype(str)
meta["ê¸°ì¤€_ë‚ ì§œ"] = pd.to_datetime(meta["ê¸°ì¤€_ë‚ ì§œ"]).dt.strftime("%Y-%m-%d")
meta["ì‹œê°„"] = meta["ì‹œê°„"].astype(int)

pred_df["ì‹œê°„"] = pred_df["ì‹œê°„"].astype(int)

pred_df = pred_df.merge(meta, on=join_cols, how="left")

# ì´ ìŠ¹ì°¨/í•˜ì°¨ ì˜ˆì¸¡ (ì‹œê°„Â·ì •ë¥˜ìž¥ ì´ëŸ‰)
pred_df["ì˜ˆì¸¡_ìŠ¹ì°¨_ì´"]  = np.clip(y_hat_board,  0, None)
pred_df["ì˜ˆì¸¡_í•˜ì°¨_ì´"] = np.clip(y_hat_alight, 0, None)

# ë²„ìŠ¤ë‹¹ ìŠ¹ì°¨/í•˜ì°¨ (0 ë¶„ëª¨ ë³´í˜¸)
den = pred_df["í†µê³¼ë²„ìŠ¤ìˆ˜"].replace(0, np.nan)
pred_df["ë²„ìŠ¤ë‹¹_ìŠ¹ì°¨"]  = (pred_df["ì˜ˆì¸¡_ìŠ¹ì°¨_ì´"]  / den).fillna(0.0)
pred_df["ë²„ìŠ¤ë‹¹_í•˜ì°¨"] = (pred_df["ì˜ˆì¸¡_í•˜ì°¨_ì´"] / den).fillna(0.0)

# ê°™ì€ (ë‚ ì§œ,ì‹œê°„) ë¸”ë¡ ë‚´ì—ì„œ ì •ë¥˜ìž¥ ìˆœì„œëŒ€ë¡œ ëˆ„ì  â†’ ë²„ìŠ¤ë‹¹ onboard ì¶”ì •
pred_df = pred_df.sort_values(["ê¸°ì¤€_ë‚ ì§œ","ì‹œê°„","ì •ë¥˜ìž¥_ìˆœì„œ"]).reset_index(drop=True)

def _accumulate_onboard(g):
    onboard = (g["ë²„ìŠ¤ë‹¹_ìŠ¹ì°¨"] - g["ë²„ìŠ¤ë‹¹_í•˜ì°¨"]).cumsum()
    onboard = onboard.clip(lower=0.0)
    g["ë²„ìŠ¤ë‹¹_onboard_ì¶”ì •"] = onboard
    return g

pred_df = pred_df.groupby(["ê¸°ì¤€_ë‚ ì§œ","ì‹œê°„"], group_keys=False).apply(_accumulate_onboard)

# ìµœì¢… ì»¬ëŸ¼ & ì •ë ¬
out = pred_df[[
    "ê¸°ì¤€_ë‚ ì§œ","ì‹œê°„","ì •ë¥˜ìž¥_ID","ì •ë¥˜ìž¥_ìˆœì„œ","ì—­ëª…","í†µê³¼ë²„ìŠ¤ìˆ˜",
    "ì˜ˆì¸¡_ìŠ¹ì°¨_ì´","ì˜ˆì¸¡_í•˜ì°¨_ì´","ë²„ìŠ¤ë‹¹_ìŠ¹ì°¨","ë²„ìŠ¤ë‹¹_í•˜ì°¨","ë²„ìŠ¤ë‹¹_onboard_ì¶”ì •"
]].copy()
out = out.sort_values(["ê¸°ì¤€_ë‚ ì§œ","ì‹œê°„","ì •ë¥˜ìž¥_ìˆœì„œ"]).reset_index(drop=True)

# ì €ìž¥
Path(os.path.dirname(OUT_PRED)).mkdir(parents=True, exist_ok=True)
out.to_csv(OUT_PRED, index=False, encoding="utf-8-sig")
print("âœ… ì˜ˆì¸¡ CSV ì €ìž¥:", OUT_PRED)